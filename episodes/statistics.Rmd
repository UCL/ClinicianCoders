---
title: "Basic Statistics"
teaching: 60
exercises: 20
source: Rmd
---

```{r, include = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE}
# Packages and data we'll use in this episode
library(tidyverse)
library(lubridate)
cchic <- read_csv("data/clean_CCHIC.csv")
```


## Content

-   Types of Data
-   Exploring your dataset
-   Descriptive Statistics
-   Inferential Statistics

## Wait!

-   Make sure `cchic` R dataframe from your work yesterday is loaded
-   Ensure this includes the variables you created including `los`

::: notes
Help them load the dataset if needed. This is the same one they used in the morning. It should include the *length of stay* variable.
:::

## The big picture

-   Research often seeks to answer a question about a larger population by collecting data on a small portion
-   Data collection:
    -   Many variables
    -   For each person/unit.
-   This procedure, *sampling*, must be controlled so as to ensure **representative** data.

## Descriptive and inferential statistics

::: Background
Just as data in general are of different types - for example numeric vs text data - statistical data are assigned to different *levels of measure* The level of measure determines how we can describe and model the data.
:::

## Data types

### Continuous

Continuous data is data that can take any value in the appropriate metric space. Examples are variables like *height* and *temperature*. Very often this is any point on the real number line. Ratio data are data that can form ratios, such that a score of seventy is twice a score of thirty five, and where the intervals between adjacent values is equal. Ratio level data also have a 'true' zero (the phenomenon measured is absent), rather than an arbitrary of calibrated zero (zero degrees Celsius is the freezing point of water.)

There is no limit in general to the mathematical operations we can carry out on continuous data.

::: Side Note It is worth noting that *interval* level data are often included as **continuous** since they can mostly be treated just like ratio data, aside from the calculation of the *coefficient of variation*, but by strict definition they are discrete. :::

### Discrete

Discrete data take their value from a closed set of possible values. There is no bound as to what the elements of the set may be - they can be integer values, fractional values, or entirely non-numeric.

Nominal data are data whose observation entails assigning them to a category from a collection of labelled categories. An example might be eye-colour. The only mathematical procedure we can use with nominal data is *counting* so as to report *frequency* of occurrence and from these frequencies, proportions. Thus the measure of location or of central location for nominal data is **the mode** and the only measure of variability or dispersion is **the variation ratio**.

Ordinal data are discrete data items that can be ranked. So, as well as counting items they can be **ordered**. The mode can almost always be calculated for ordinal data, but it may also be possible to calculate the **median** as an alternative measure of central location. Variability in ordinal data is indicated by **range** and **interquartile range**.

Interval

Interval data are numeric data that are ordered on a scale where the distance between measuring points is equal. Unlike ratio data, the zero point on an interval scale is an arbitrarily chosen calibration and data measurements do not form ratios.

## Continuous variables

::: columns
::: column
-   e.g. age, height, weight
-   Have distributions:
    -   Gaussian
    -   Poisson
    -   Binomial
    -   Cauchy/Lorenz
-   Can't be described
:::

::: column
![](../Images/ContinuousDistribution.png)
:::
:::

## What is normally distributed data?

![](../Images/NormalDistribution.png)

## Types of discrete variables

-   Nominal
    -   e.g. hair colour, types of antibiotics
    -   There is no order between the data types (e.g. blonde, brunette, red hair)

## Types of discrete variables

::: columns
::: column
-   Ordinal
    -   There is an order e.g. `care_level` where Level 3 \> Level 2 \> Level 1 etc.
    -   However, the difference between Level 1 and Level 2 critical care may not be the same as the difference between Level 2 and Level 3.
:::

::: column
![](../Images/OrdinalData.png)
:::
:::

## Types of discrete data

::: columns
::: column
-   Interval
    -   There is an order to data points (e.g. `age_cat` for age centile) and the difference between these points are equal (e.g. 10 years)
:::

::: column
![](../Images/IntervalData.png)
:::
:::

## Describing data

-   Continuous variables
-   Discrete variables

::: notes
How do we convey information on what your data looks like, using numbers or figures?
:::

## Describing continuous data.

::: columns
::: column
First establish the distribution of the data. You can visualise this with a histogram.

```{r, eval = F}
ggplot(cchic, aes(x = age_years)) +
  geom_histogram()
```

What is the distribution of this data?
:::

::: column
![](../Images/hist_age.png)
:::
:::

## What is the distribution of height?

::: columns
::: column
Try this command

```{r, eval = FALSE}
ggplot(data = cchic, aes(x = height)) +
  geom_histogram()
```

What is the distribution of this data?
:::

::: column
![](../Images/hist_height.png)
:::
:::

## Parametric vs non-parametric analysis

-   Parametric analysis assumes that
    -   The data follows a known distribution
    -   It can be described using *parameters*
    -   Examples of distributions include, normal, Poisson, exponential.
-   Non parametric data
    -   The data can't be said to follow a known distribution

::: notes
Emphasise that parametric is not equal to normal.
:::

## Describing parametric and non-parametric data

How do you use numbers to convey what your data looks like.

-   Parametric data
    -   Use the parameters that describe the distribution.
    -   For a Gaussian (normal) distribution - use mean and standard deviation
    -   For a Poisson distribution - use average event rate
    -   etc.
-   Non Parametric data
    -   Use the median (the middle number when they are ranked from lowest to highest) and the interquartile range (the number 75% of the way up the list when ranked minus the number 25% of the way)
-   You can use the command `summary(data_frame_name)` to get these numbers for each variable.

## Mean versus standard deviation

-   What does standard deviation mean?
-   Both graphs have the same mean (center), but the second one has data which is more spread out.

```{r, eval = FALSE}

# small standard deviation
dummy_1 <- rnorm(1000, mean = 10, sd = 0.5)
dummy_1 <- as.data.frame(dummy_1)
ggplot(dummy_1, aes(x = dummy_1)) +
  geom_histogram()

# larger standard deviation
dummy_2 <- rnorm(1000, mean = 10, sd = 20)
dummy_2 <- as.data.frame(dummy_2)
ggplot(dummy_2, aes(x = dummy_2)) +
  geom_histogram()
```

::: notes
Get them to plot the graphs. Explain that we are generating random data from different distributions and plotting them.
:::

## Calculating mean and standard deviation

```{r, eval = T}
mean(cchic$height, na.rm = TRUE)
```

Calculate the standard deviation and confirm that it is the square root of the variance:

```{r, eval = T}
sdheight = sd(cchic$height, na.rm = TRUE)

varheight = var(cchic$height, na.rm = TRUE)

sqrt(varheight) == sdheight

```

The `na.rm` argument tells R to ignore missing values in the variable.

## Calculating median and interquartile range

```{r, eval = T}
median(cchic$age_years, na.rm = TRUE)
```

```{r, eval = T}
IQR(cchic$age_years, na.rm = TRUE)
```

Again, we ignore the missing values.

## Describing discrete data

-   Frequencies

```{r, eval = T}
table(cchic$vital_status)
```

-   Proportions

```{r, eval = T}
status <- table(cchic$vital_status)
prop.table(status)
```

Contingency tables of frequencies can also be tabulated with **table()**. For example:

```{r}
table(cchic$vital_status, cchic$chemo)
```

Which leads quite naturally to the consideration of any association between the observed frequencies.

## Inferential statistics

## Meaningful analysis

-   What is your hypothesis - what is your null hypothesis?

::: Note
Always: the level of the independent variable has no effect on the level of the dependent variable.
:::

-   What type of variables (data type) do you have?

-   What are the assumptions of the test you are using?

-   Interpreting the result

## What is a p-value?

![](../Images/pValue.png)

[The Lady Tasting Tea](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/cem.3239)

Fisher informally characterised the *p-value* as an estimate of the strength of the evidence against the null hypothesis [^1] (Dahiru T. (2008). P - value, a true test of statistical significance? A cautionary note. Annals of Ibadan postgraduate medicine, 6(1), 21--26. <https://doi.org/10.4314/aipm.v6i1.64038>)

[^1]: Dahiru T. (2008). P - value, a true test of statistical significance? A cautionary note. *Annals of Ibadan postgraduate medicine*, *6*(1), 21--26. <https://doi.org/10.4314/aipm.v6i1.64038>

Depending on the data under consideration and our purpose, we choose a threshold of evidence for the rejection of the null hypothesis. For many purposes, a confidence level of 95% is chosen, which means that a p-value of less than 0.05 is sufficient to reject the null hypothesis. In other circumstances and confidence level of 99% may be appropriate, or even higher.

## What we are usually hoping...

![](../Images/pValue2.png)

## Testing significance

-   p-value

-   \<0.05

-   0.03-0.049

    -   Would benefit from further testing.

**0.05** is not a magic number.

## Comparing means

It all starts with a hypothesis

-   Null hypothesis
    -   "There is no difference in mean height between men and women" $$mean\_height\_men - mean\_height\_women = 0$$
-   Alternate hypothesis
    -   "There is a difference in mean height between men and women"

## More on hypothesis testing

-   The null hypothesis (H0) assumes that the true mean difference (μd) is equal to zero.

-   The two-tailed alternative hypothesis (H1) assumes that μd is not equal to zero.

-   The upper-tailed alternative hypothesis (H1) assumes that μd is greater than zero.

-   The lower-tailed alternative hypothesis (H1) assumes that μd is less than zero.

-   Remember: hypotheses are never about data, they are about the processes which produce the data. The value of μd is unknown. The goal of hypothesis testing is to determine the hypothesis (null or alternative) with which the data are more consistent.

## Comparing means

Is there an absolute difference between the heights of males and females?

```{r}
cchic %>%
  group_by(sex) %>%
  summarise(av.height = mean(height, na.rm = TRUE))
```

Is the difference between heights statistically significant?

## t-test

::: columns
::: column
-   Compares means between two populations
-   Paired vs. Unpaired
:::

::: column
![](../Images/Ttest.png)
:::
:::

## Assumptions of a t-test

-   One independent categorical variable with 2 groups and one dependent continuous variable

-   The dependent variable is approximately normally distributed in each group

-   The observations are independent of each other

-   For students' original t-statistic, that the variances in both groups are more or less equal. This constraint should probably be abandoned in favour of always using a conservative test.

## Doing a t-test

```{r, eval = FALSE}
t.test(height ~ sex, data = cchic)
```

::: notes
Tell them to do this. The default for t.test() is that variances are unequal and it produces Welch's statistic.
:::

## Doing the t-test

```{r, echo = FALSE}
t.test(height ~ sex, data = cchic)
```

::: notes
Quickly explain the main points of the output
:::

## Comparing counts

-   Is survival different between genders?

```{r}
table(cchic$sex, cchic$vital_status)
```

## What is our hypothesis?

-   Null hypothesis
    -   There is no difference in survival between men and women
-   Alternate hypothesis
    -   There is a difference in survival between men and women

## Assumptions of the chi-squared test.

1.  Data in cells should be frequencies or counts *not* percentages
2.  Levels/Categories are mutually exclusive -- here being a alive/dead applies
3.  Each subject contributes to one cell -- can either be male/female and alive/dead
4.  Independent study groups
5.  2 categorical variables
6.  Expected values in no more than 20% of cells are \< 5

`Biochem Med (Zagreb). 2013 Jun; 23(2): 143–149.`

## Doing the chi-squared test.

Start with `?chisq.test`. Then do the test.

```{r}
x <- chisq.test(cchic$sex, cchic$vital_status)
summary(x)
```

::: notes (important as it holds for results of all statistical tests) Get them to do this, then explain the output. Do the same test, assigning the result to a data object and explain how to access the contents of the data object to show, df, expected values, residuals etc. :::

## Non paramteric versions

Is length of stay different between genders?

```{r, message=FALSE}
ggplot(data = cchic, aes(x = los)) +
  geom_histogram() +
  facet_grid(~sex)
```

## When do you use a non-parametric test?

-   When any of the the following are true.
    -   Level of measurement is nominal or ordinal
    -   Unequal sample sizes
    -   Skewed data
    -   Unequal variance
    -   Continuous data collapsed into small number of categories

## Using the Mann Whitney test

`??Mann-Whitney` will show you that the command is actually called `wilcox.test`.t

```{r}
wilcox.test(los ~ sex, data = cchic)
```

::: notes
Explain the output
:::

## more than two levels of IV

While the t-test is sufficient where there are two levels of the IV, for situations where there are more than two, we use the **ANOVA** family of procedures. To show this, we will create a variable that subsets our data by *creatinine* levels. If the ANOVA result is statistically significant, we will use a post-hoc test method to do pairwise comparisons (here Tukey's Honest Significant Differences.)

```{r}
quantile(cchic$creatinine)
IQR(cchic$creatinine)

cchic$creatininegroup <- cut(cchic$creatinine, breaks = c(15, 64, 83, 123, 1533), labels = FALSE)

# cchic %>%
#  mutate(creatininegroup = cut(cchic$creatinine, breaks=c(15, 64, 83, 123,1533), labels = FALSE))

cchic$creatininegroup <- factor(cchic$creatininegroup)

anovamodel <- aov(cchic$ph_abg ~ cchic$creatininegroup)
summary(anovamodel)

TukeyHSD(anovamodel)
```

# Regression Modelling

The most common use of regression modelling is to explore the relationship between two continuous variables, for example between `temp_c` and `temp_nc` in our data. We can first determine whether there is any significant correlation between the values, and if there is, plot the relationship.

```{r}
cor.test(cchic$temp_c, cchic$temp_nc)

ggplot(cchic, aes(temp_c, temp_nc)) +
  geom_point() +
  geom_smooth(method = "lm")

```

Having decided that a further investigation of this relationship is worthwhile, we can create a linear model with the function `lm()`

``` {r}
modelone <- lm(cchic$temp_nc ~ cchic$temp_c)
summary(modelone)

```

## Regression with a categorical IV (the t-test)

Run the following code chunk and compare the results to the t test conducted earlier.

```{r}
cchic %>%
  mutate(sex = factor(sex))

modelttest <- lm(cchic$height ~ cchic$sex)

summary(modelttest)

```

## Regression with a categorical IV (ANOVA)

Use the `lm()` function to model the relationship between `cchic$creatininegroup` and `cchic$ph_abg`.  Compare the results with the ANOVA carried out earlier.
